<!doctype html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.19" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.66" />
    <style>
      :root {
        --vp-c-bg: #fff;
      }

      [data-theme="dark"] {
        --vp-c-bg: #1b1b1f;
      }

      html,
      body {
        background: var(--vp-c-bg);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <meta property="og:url" content="https://xiucaistack.cn/AI%E8%BF%9B%E9%98%B6%E4%B9%8B%E8%B7%AF/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/LangChain%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/Memery.html"><meta property="og:site_name" content="秀才的进阶之路"><meta property="og:title" content="Memory"><meta property="og:description" content="Memory 1. 什么是 LangChain Memory LangChain Memory 是 LangChain 框架中的 记忆（Memory）管理组件，用于 存储和管理会话历史，从而让 LLM（大语言模型）在多轮对话中保持上下文。简单来说，它的主要作用是给语言模型应用添加“记忆力”。使得应用能够“记住”之前的交互，从而提供更智能、更自然的对话体..."><meta property="og:type" content="article"><meta property="og:locale" content="zh-CN"><meta property="og:updated_time" content="2025-06-26T15:42:17.000Z"><meta property="article:tag" content="AI"><meta property="article:tag" content="AI应用开发"><meta property="article:tag" content="llm"><meta property="article:tag" content="大模型"><meta property="article:tag" content="大模型应用开发"><meta property="article:tag" content="LangChain"><meta property="article:tag" content="Data Connection"><meta property="article:tag" content="Memory"><meta property="article:tag" content="记忆"><meta property="article:modified_time" content="2025-06-26T15:42:17.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"Memory","image":[""],"dateModified":"2025-06-26T15:42:17.000Z","author":[{"@type":"Person","name":"秀才","url":"https://github.com/Percygu"}]}</script><link rel="icon" href="/web_logo2.png"><title>Memory | 秀才的进阶之路</title><meta name="description" content="Memory 1. 什么是 LangChain Memory LangChain Memory 是 LangChain 框架中的 记忆（Memory）管理组件，用于 存储和管理会话历史，从而让 LLM（大语言模型）在多轮对话中保持上下文。简单来说，它的主要作用是给语言模型应用添加“记忆力”。使得应用能够“记住”之前的交互，从而提供更智能、更自然的对话体...">
    <link rel="preload" href="/assets/style-CKsNRfGx.css" as="style"><link rel="stylesheet" href="/assets/style-CKsNRfGx.css">
    <link rel="modulepreload" href="/assets/app-DJgFtDFQ.js"><link rel="modulepreload" href="/assets/Memery.html-D8YeWBKl.js"><link rel="modulepreload" href="/assets/plugin-vue_export-helper-DlAUqK2U.js">
    <link rel="prefetch" href="/assets/index.html-hr-5arCi.js" as="script"><link rel="prefetch" href="/assets/2025 年你必须知道的 10 个向量数据库.html-ss-fioaa.js" as="script"><link rel="prefetch" href="/assets/Cusor AI编程实战(1)：抖音爆款文案提取_改写工具(上).html-BbwJy2az.js" as="script"><link rel="prefetch" href="/assets/Claude 4发布，程序员真的要失业了吗.html-B8hL9MeH.js" as="script"><link rel="prefetch" href="/assets/Cursor 1.0 终于来了.html-DNfTAr1U.js" as="script"><link rel="prefetch" href="/assets/一文讲透大模型应用开发：你必须掌握的新时代技术核心竞争力.html-DbAwD2cd.js" as="script"><link rel="prefetch" href="/assets/人工智能导论.html-0DJnWJbA.js" as="script"><link rel="prefetch" href="/assets/什么是大语言模型.html-CXCH9qrK.js" as="script"><link rel="prefetch" href="/assets/理解机器学习.html-DSXYphsh.js" as="script"><link rel="prefetch" href="/assets/理解深度学习.html-ANhYQupq.js" as="script"><link rel="prefetch" href="/assets/生成式AI简介.html-BYAVjdRa.js" as="script"><link rel="prefetch" href="/assets/Go环境搭建.html-CEvm36p5.js" as="script"><link rel="prefetch" href="/assets/Go编码规范.html-8tWMdcpd.js" as="script"><link rel="prefetch" href="/assets/Go语言前景.html-D7CoWx3z.js" as="script"><link rel="prefetch" href="/assets/Go语言单测.html-C-AhRPeA.js" as="script"><link rel="prefetch" href="/assets/channel原理.html-DXrvUT4_.js" as="script"><link rel="prefetch" href="/assets/context原理.html-B2mQGoGv.js" as="script"><link rel="prefetch" href="/assets/defer原理.html-Y9MK_G-I.js" as="script"><link rel="prefetch" href="/assets/gmp调度原理.html-_wNU5p8A.js" as="script"><link rel="prefetch" href="/assets/interface原理.html-BaxVgOM4.js" as="script"><link rel="prefetch" href="/assets/map原理.html-JATBkwC0.js" as="script"><link rel="prefetch" href="/assets/slice原理.html-BR0rlnqN.js" as="script"><link rel="prefetch" href="/assets/string原理.html-pPOVu6J8.js" as="script"><link rel="prefetch" href="/assets/sync.map原理.html-Ob4fxQbS.js" as="script"><link rel="prefetch" href="/assets/内存管理.html-BMAGHwTF.js" as="script"><link rel="prefetch" href="/assets/垃圾回收.html-Cn6P-ELc.js" as="script"><link rel="prefetch" href="/assets/程序初始化.html-Dv3L2bAR.js" as="script"><link rel="prefetch" href="/assets/逃逸分析.html-Bwjw3Lig.js" as="script"><link rel="prefetch" href="/assets/Go语言Map.html-Hewt9haG.js" as="script"><link rel="prefetch" href="/assets/Go语言defer.html-Dfo6qdxK.js" as="script"><link rel="prefetch" href="/assets/Go语言error.html-BPZAwS-b.js" as="script"><link rel="prefetch" href="/assets/Go语言代码结构.html-DGCBXux2.js" as="script"><link rel="prefetch" href="/assets/Go语言依赖管理.html-DmGAg2rI.js" as="script"><link rel="prefetch" href="/assets/Go语言函数.html-c2tcrfUE.js" as="script"><link rel="prefetch" href="/assets/Go语言变量.html-CpbGz5dm.js" as="script"><link rel="prefetch" href="/assets/Go语言命名规范.html-BGBJuKmw.js" as="script"><link rel="prefetch" href="/assets/Go语言常量.html-DvkdYnlP.js" as="script"><link rel="prefetch" href="/assets/Go语言异常捕获.html-Br0DnROq.js" as="script"><link rel="prefetch" href="/assets/Go语言循环.html-Fx-WXSQN.js" as="script"><link rel="prefetch" href="/assets/Go语言指针.html-BXZnl7p0.js" as="script"><link rel="prefetch" href="/assets/Go语言接口.html-D-Lu05HF.js" as="script"><link rel="prefetch" href="/assets/Go语言数组与切片.html-BFGjJFEJ.js" as="script"><link rel="prefetch" href="/assets/Go语言方法.html-BbK7Y7vq.js" as="script"><link rel="prefetch" href="/assets/Go语言条件句.html-Bpcqd8Om.js" as="script"><link rel="prefetch" href="/assets/Go语言结构体.html-BJALtI7y.js" as="script"><link rel="prefetch" href="/assets/Go语言运算符.html-BHDSJdRU.js" as="script"><link rel="prefetch" href="/assets/gin.html-BI-jV4Ai.js" as="script"><link rel="prefetch" href="/assets/gorm.html-CjX309PV.js" as="script"><link rel="prefetch" href="/assets/Channel.html-Qig4kVDr.js" as="script"><link rel="prefetch" href="/assets/Context.html-Cky7wbaD.js" as="script"><link rel="prefetch" href="/assets/Goroutine.html-ejw7K0VT.js" as="script"><link rel="prefetch" href="/assets/Select.html-B1QgYDRJ.js" as="script"><link rel="prefetch" href="/assets/Sync.html-DeKW7F-E.js" as="script"><link rel="prefetch" href="/assets/协程池.html-BFaKvm37.js" as="script"><link rel="prefetch" href="/assets/反射.html-CgENphPD.js" as="script"><link rel="prefetch" href="/assets/定时器.html-cvrehHvO.js" as="script"><link rel="prefetch" href="/assets/并发概述.html-Bdnmrlbc.js" as="script"><link rel="prefetch" href="/assets/范型.html-tels-3NM.js" as="script"><link rel="prefetch" href="/assets/Channel面试题.html-C7WjqjqL.js" as="script"><link rel="prefetch" href="/assets/Context面试题.html-CtIUsGqe.js" as="script"><link rel="prefetch" href="/assets/GMP面试题.html-DqgPGxDT.js" as="script"><link rel="prefetch" href="/assets/Interface面试题.html-Dfxg7Fe4.js" as="script"><link rel="prefetch" href="/assets/Map面试题.html-DnX_JTAb.js" as="script"><link rel="prefetch" href="/assets/Slice面试题.html-a6Q99Efx.js" as="script"><link rel="prefetch" href="/assets/Sync面试题.html-Bb-ZEb6q.js" as="script"><link rel="prefetch" href="/assets/代码面试题.html-WOnr829S.js" as="script"><link rel="prefetch" href="/assets/内存管理面试题.html-D7qBHPQJ.js" as="script"><link rel="prefetch" href="/assets/反射面试题.html-BOCwpGB2.js" as="script"><link rel="prefetch" href="/assets/垃圾回收面试题.html-DT27zOcS.js" as="script"><link rel="prefetch" href="/assets/基础面试题.html-Ck579Iqg.js" as="script"><link rel="prefetch" href="/assets/mysql.html-DSXz6jAd.js" as="script"><link rel="prefetch" href="/assets/mysql1.html-EcsMT5CG.js" as="script"><link rel="prefetch" href="/assets/redis.html-BjcpBxaw.js" as="script"><link rel="prefetch" href="/assets/Go程序数据库连接池耗尽如何排查.html-Dh-46Wnb.js" as="script"><link rel="prefetch" href="/assets/Java线上接口响应慢如何排查.html-CSkVeF7J.js" as="script"><link rel="prefetch" href="/assets/当面试官问起_负载均衡_，哪些点是核心？.html-DUMeJfrf.js" as="script"><link rel="prefetch" href="/assets/微服务架构核心：服务注册与发现的AP与CP抉择.html-CNk4msVA.js" as="script"><link rel="prefetch" href="/assets/服务降级：从有损服务到保障核心业务的架构智慧.html-Dpj852AN.js" as="script"><link rel="prefetch" href="/assets/熔断：如何优雅地应对服务雪崩与抖动.html-BalP3Ttk.js" as="script"><link rel="prefetch" href="/assets/限流：从算法到阈值，一次性讲透.html-C8r4LBVy.js" as="script"><link rel="prefetch" href="/assets/消息丢失.html-BhVoAyi0.js" as="script"><link rel="prefetch" href="/assets/Agent.html-DYkKkJvP.js" as="script"><link rel="prefetch" href="/assets/LCEL.html-Bf-5cx_F.js" as="script"><link rel="prefetch" href="/assets/LangChain基础概念.html-AIyCBb0n.js" as="script"><link rel="prefetch" href="/assets/Model-IO.html-QXSSE93n.js" as="script"><link rel="prefetch" href="/assets/RAG(检索增强).html-BVckW6AA.js" as="script"><link rel="prefetch" href="/assets/回调.html-BiAxp4AV.js" as="script"><link rel="prefetch" href="/assets/数据连接.html-C2cHXPYh.js" as="script"><link rel="prefetch" href="/assets/流式传输.html-BlWTaveY.js" as="script"><link rel="prefetch" href="/assets/链.html-D4Y5PLML.js" as="script"><link rel="prefetch" href="/assets/RAG总体概览.html-Db2q9hJE.js" as="script"><link rel="prefetch" href="/assets/大模型简介.html-BV3CKElt.js" as="script"><link rel="prefetch" href="/assets/404.html-CaogHyD5.js" as="script"><link rel="prefetch" href="/assets/index.html-DV0nuR_e.js" as="script"><link rel="prefetch" href="/assets/index.html-BTb2SfUN.js" as="script"><link rel="prefetch" href="/assets/index.html-C-j0cxRw.js" as="script"><link rel="prefetch" href="/assets/index.html-BQ18GZLP.js" as="script"><link rel="prefetch" href="/assets/index.html-B7RtAKXr.js" as="script"><link rel="prefetch" href="/assets/index.html-C4mAwwnd.js" as="script"><link rel="prefetch" href="/assets/index.html-CbDXGqig.js" as="script"><link rel="prefetch" href="/assets/index.html-C2SxXsMT.js" as="script"><link rel="prefetch" href="/assets/index.html-DRHA1Jxa.js" as="script"><link rel="prefetch" href="/assets/index.html-BuFn3Fv8.js" as="script"><link rel="prefetch" href="/assets/index.html-BylljNRo.js" as="script"><link rel="prefetch" href="/assets/index.html-Ciz6I5h6.js" as="script"><link rel="prefetch" href="/assets/index.html-DI3tXa0B.js" as="script"><link rel="prefetch" href="/assets/index.html-CEA2N0lf.js" as="script"><link rel="prefetch" href="/assets/index.html-DQ40-46Q.js" as="script"><link rel="prefetch" href="/assets/index.html-DrWkG-mt.js" as="script"><link rel="prefetch" href="/assets/index.html-qZ5ClSXg.js" as="script"><link rel="prefetch" href="/assets/index.html-Dh99Cdv6.js" as="script"><link rel="prefetch" href="/assets/index.html-Ltf3cUZ_.js" as="script"><link rel="prefetch" href="/assets/index.html-CAPw4g0O.js" as="script"><link rel="prefetch" href="/assets/index.html-Br18vfTa.js" as="script"><link rel="prefetch" href="/assets/index.html-lCXYz_Ap.js" as="script"><link rel="prefetch" href="/assets/index.html-de6-uN2n.js" as="script"><link rel="prefetch" href="/assets/index.html-CjciaFsh.js" as="script"><link rel="prefetch" href="/assets/index.html-khC3UggJ.js" as="script"><link rel="prefetch" href="/assets/index.html-Bx_k-c9j.js" as="script"><link rel="prefetch" href="/assets/photoswipe.esm-GXRgw7eJ.js" as="script"><link rel="prefetch" href="/assets/giscus-BZxmVUME.js" as="script"><link rel="prefetch" href="/assets/index-Dm7E2SdV.js" as="script"><link rel="prefetch" href="/assets/setupDevtools-7MC2TMWH-DAzHrGVH.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">跳至主要內容</a><!--]--><div class="theme-container external-link-icon has-toc" vp-container><!--[--><header id="navbar" class="vp-navbar" vp-navbar><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><a class="route-link vp-brand" href="/" aria-label="带我回家"><img class="vp-nav-logo" src="/web_logo2.png" alt><!----><span class="vp-site-name hide-in-pad">秀才的进阶之路</span></a><!--]--></div><div class="vp-navbar-center"><!--[--><nav class="vp-nav-links"><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/" aria-label="秀才的进阶之路"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-home" style=""></span><!--]-->秀才的进阶之路<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/Go%E8%AF%AD%E8%A8%80%E7%B3%BB%E5%88%97/Go%E8%AF%AD%E8%A8%80%E5%89%8D%E6%99%AF/Go%E8%AF%AD%E8%A8%80%E5%89%8D%E6%99%AF.html" aria-label="Go语言进阶之路"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-book" style=""></span><!--]-->Go语言进阶之路<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/%E5%90%8E%E7%AB%AF%E8%BF%9B%E9%98%B6%E4%B9%8B%E8%B7%AF/%E9%9D%A2%E8%AF%95%E5%9C%BA%E6%99%AF%E9%A2%98/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E6%A0%B8%E5%BF%83%EF%BC%9A%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C%E4%B8%8E%E5%8F%91%E7%8E%B0%E7%9A%84AP%E4%B8%8ECP%E6%8A%89%E6%8B%A9.html" aria-label="后端进阶之路"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-chart-simple" style=""></span><!--]-->后端进阶之路<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/AI%E8%BF%9B%E9%98%B6%E4%B9%8B%E8%B7%AF/%E7%94%9F%E6%88%90%E5%BC%8FAI%E5%85%A5%E9%97%A8/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AF%BC%E8%AE%BA.html" aria-label="AI进阶之路"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-robot" style=""></span><!--]-->AI进阶之路<!----></a></div></nav><!--]--></div><div class="vp-navbar-end"><!--[--><!----><div class="vp-nav-item vp-action"><a class="vp-action-link" href="https://github.com/yourusername/xiucai-stack" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" name="github" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="vp-nav-item hide-in-mobile"><button type="button" class="vp-color-mode-switch" id="color-mode-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" name="auto" style="display:block;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" name="dark" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="light icon" name="light" style="display:none;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><!--[--><div id="docsearch-container" style="display:none;"></div><div><button type="button" class="DocSearch DocSearch-Button" aria-label="搜索文档"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">搜索文档</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"><svg width="15" height="15" class="DocSearch-Control-Key-Icon"><path d="M4.505 4.496h2M5.505 5.496v5M8.216 4.496l.055 5.993M10 7.5c.333.333.5.667.5 1v2M12.326 4.5v5.996M8.384 4.496c1.674 0 2.116 0 2.116 1.5s-.442 1.5-2.116 1.5M3.205 9.303c-.09.448-.277 1.21-1.241 1.203C1 10.5.5 9.513.5 8V7c0-1.57.5-2.5 1.464-2.494.964.006 1.134.598 1.24 1.342M12.553 10.5h1.953" stroke-width="1.2" stroke="currentColor" fill="none" stroke-linecap="square"></path></svg></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar" vp-sidebar><!----><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable active" type="button"><!----><span class="vp-sidebar-title">AI进阶之路</span><span class="vp-arrow down"></span></button><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">生成式AI入门指南</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">提示词工程</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable active" type="button"><!----><span class="vp-sidebar-title">大模型应用开发</span><span class="vp-arrow down"></span></button><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">大模型导论</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable active" type="button"><!----><span class="vp-sidebar-title">LangChain从入门到精通</span><span class="vp-arrow down"></span></button><ul class="vp-sidebar-links"><li><a class="route-link auto-link vp-sidebar-link" href="/AI%E8%BF%9B%E9%98%B6%E4%B9%8B%E8%B7%AF/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/LangChain%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/LangChain%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5.html" aria-label="LangChain基础概念"><!---->LangChain基础概念<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/AI%E8%BF%9B%E9%98%B6%E4%B9%8B%E8%B7%AF/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/LangChain%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/Model-IO.html" aria-label="Model I/O"><!---->Model I/O<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/AI%E8%BF%9B%E9%98%B6%E4%B9%8B%E8%B7%AF/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/LangChain%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/%E6%95%B0%E6%8D%AE%E8%BF%9E%E6%8E%A5.html" aria-label="数据连接"><!---->数据连接<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/AI%E8%BF%9B%E9%98%B6%E4%B9%8B%E8%B7%AF/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/LangChain%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/%E9%93%BE.html" aria-label="链"><!---->链<!----></a></li><li><a class="route-link route-link-active auto-link vp-sidebar-link active" href="/AI%E8%BF%9B%E9%98%B6%E4%B9%8B%E8%B7%AF/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/LangChain%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/Memery.html" aria-label="Memory"><!---->Memory<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/AI%E8%BF%9B%E9%98%B6%E4%B9%8B%E8%B7%AF/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/LangChain%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/RAG(%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA).html" aria-label="RAG(检索增强)"><!---->RAG(检索增强)<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/AI%E8%BF%9B%E9%98%B6%E4%B9%8B%E8%B7%AF/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/LangChain%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/Agent.html" aria-label="Agent"><!---->Agent<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/AI%E8%BF%9B%E9%98%B6%E4%B9%8B%E8%B7%AF/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/LangChain%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/%E5%9B%9E%E8%B0%83.html" aria-label="Callbacks"><!---->Callbacks<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/AI%E8%BF%9B%E9%98%B6%E4%B9%8B%E8%B7%AF/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/LangChain%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/LCEL.html" aria-label="LCEL"><!---->LCEL<!----></a></li></ul></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">RAG系列</span><span class="vp-arrow end"></span></button><!----></section></li></ul></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">AI编程</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">AI应用</span><span class="vp-arrow end"></span></button><!----></section></li></ul></section></li></ul><!----></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->Memory</h1><div class="page-info"><span class="page-author-info" aria-label="作者🖊" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon" name="author"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><a class="page-author-item" href="https://github.com/Percygu" target="_blank" rel="noopener noreferrer">秀才</a></span><span property="author" content="秀才"></span></span><!----><span class="page-date-info" aria-label="写作日期📅" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon" name="calendar"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span data-allow-mismatch="text">2025年6月26日</span><meta property="datePublished" content="2025-06-26T15:42:17.000Z"></span><!----><span class="page-reading-time-info" aria-label="阅读时间⌛" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon" name="timer"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>大约 25 分钟</span><meta property="timeRequired" content="PT25M"></span><!----><span class="page-tag-info" aria-label="标签🏷" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon" name="tag"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item color5" role>AI</span><span class="page-tag-item color3" role>AI应用开发</span><span class="page-tag-item color3" role>llm</span><span class="page-tag-item color4" role>大模型</span><span class="page-tag-item color1" role>大模型应用开发</span><span class="page-tag-item color4" role>LangChain</span><span class="page-tag-item color6" role>Data Connection</span><span class="page-tag-item color1" role>Memory</span><span class="page-tag-item color7" role>记忆</span><!--]--><meta property="keywords" content="AI,AI应用开发,llm,大模型,大模型应用开发,LangChain,Data Connection,Memory,记忆"></span></div><hr></div><div class="vp-toc-placeholder"><aside id="toc" vp-toc><!----><div class="vp-toc-header">此页内容<button type="button" class="print-button" title="打印"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon" name="print"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button><div class="arrow end"></div></div><div class="vp-toc-wrapper"><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_1-什么是-langchain-memory">1. 什么是 LangChain Memory</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_2-为什么需要-memory">2. 为什么需要 Memory</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_3-conversationchain">3. ConversationChain</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_3-1-conversationchain的主要组成部分">3.1 ConversationChain的主要组成部分</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_3-2-conversationchain的内置模板">3.2 ConversationChain的内置模板</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_4-memory组件">4. Memory组件</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_4-1-conversationbuffermemory">4.1 ConversationBufferMemory</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_4-2-conversationbufferwindowmemory">4.2 ConversationBufferWindowMemory</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_4-3-conversationsummarymemory">4.3 ConversationSummaryMemory</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_4-4-conversationsummarybuffermemory">4.4 ConversationSummaryBufferMemory</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_4-5-其他会话记忆组件">4.5 其他会话记忆组件</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_5-小结">5. 小结</a></li><!----><!--]--></ul><div class="vp-toc-marker" style="top:-1.7rem;"></div></div><!----></aside></div><!----><div class="theme-hope-content" vp-content><h1 id="memory" tabindex="-1"><a class="header-anchor" href="#memory"><span>Memory</span></a></h1><h2 id="_1-什么是-langchain-memory" tabindex="-1"><a class="header-anchor" href="#_1-什么是-langchain-memory"><span>1. 什么是 LangChain Memory</span></a></h2><p>LangChain Memory 是 LangChain 框架中的 <strong>记忆（Memory）管理组件</strong>，用于 <strong>存储和管理会话历史</strong>，从而让 LLM（大语言模型）在多轮对话中保持上下文。简单来说，它的主要作用是<strong>给语言模型应用添加“记忆力”</strong>。使得应用能够“记住”之前的交互，从而提供更智能、更自然的对话体验。</p><figure><img src="/assets/image-D34gm2EQ.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="_2-为什么需要-memory" tabindex="-1"><a class="header-anchor" href="#_2-为什么需要-memory"><span>2. 为什么需要 Memory</span></a></h2><p>默认情况下，LLM 是 <strong>无状态的</strong>，每次调用都是独立的，也就是每次调用都不会记住之前的对话。这种无状态性在需要连续交互的应用（如聊天机器人）中，会导致不少问题，比如<strong>对话不连贯，</strong>每次提问都需要提供完整的背景信息，否则模型的回答可能会脱离上下文，显得前言不搭后语。而直接把所有历史对话传递给 LLM 又可能导致 Token 超出限制。</p><p>所以，引入 Memory 组件，LLM 就能“知道”之前的对话内容，理解当前的交互是在什么背景下发生的，更智能，更准确的提供回复</p><h2 id="_3-conversationchain" tabindex="-1"><a class="header-anchor" href="#_3-conversationchain"><span>3. ConversationChain</span></a></h2><p>我们可以以多种方式实现会话记忆，但是它们都是建立在 ConversationChain 之上的。所以在学习这些具体的Memory机制之前，需要先了解一下<strong>ConversationChain。</strong></p><p>在上一章节，我们学习了部分LangChain中的链结构，ConversationChain其实也是属于LangChain中链结构的一种。ConversationChain 是 LangChain 提供的一个基础对话链（Chain），用于处理简单的多轮对话。这个 Chain 最主要的特点就是它提供了包含 AI 前缀和人类前缀的对话摘要格式，这个对话格式能够和记忆机制紧密结合，把上下文作为提示的一部分，在最新的调用中传递给大语言模型，从而让大模型具有记忆功能</p><h3 id="_3-1-conversationchain的主要组成部分" tabindex="-1"><a class="header-anchor" href="#_3-1-conversationchain的主要组成部分"><span>3.1 ConversationChain的主要组成部分</span></a></h3><p>ConversationChain主要由以下三个部分组成：</p><ol><li><p><strong>LLM（大语言模型）</strong>：用于生成对话回复，如 OpenAI 的 GPT-4、DeepSeek 等</p></li><li><p><strong>Memory（记忆机制）</strong>：存储对话历史，使模型在多轮对话中具备上下文理解能力，避免“失忆”</p></li><li><p><strong>Prompt 模板</strong>：定义对话格式，确保模型能正确理解输入和输出</p></li></ol><h3 id="_3-2-conversationchain的内置模板" tabindex="-1"><a class="header-anchor" href="#_3-2-conversationchain的内置模板"><span>3.2 ConversationChain的内置模板</span></a></h3><p>看一个具体的示例，并打印出 ConversationChain 中的内置提示模板</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> langchain_deepseek </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> ChatDeepSeek </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 替换为 DeepSeek 模型</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> dotenv </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> load_dotenv</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> os</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">load_dotenv</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">()</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 创建 DeepSeek 模型实例</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">llm </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> ChatDeepSeek</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    model</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;deepseek-chat&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    temperature</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">0</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    api_key</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">os.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">getenv</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;DEEPSEEK_API_KEY&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">#初始化对话链</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> langchain.chains </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> ConversationChain</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">conv_chain </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> ConversationChain</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    llm</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">llm,</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">#打印对话的模型</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(conv_chain.prompt.template)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>程序输出</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">The following </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">is</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> a friendly conversation between a human </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">and</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> an </span><span style="--shiki-light:#383A42;--shiki-dark:#D19A66;">AI</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">. The </span><span style="--shiki-light:#383A42;--shiki-dark:#D19A66;">AI</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;"> is</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> talkative </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">and</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> provides lots of specific details </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> its context. If the </span><span style="--shiki-light:#383A42;--shiki-dark:#D19A66;">AI</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> does </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">not</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> know the answer to a question, it truthfully says it does </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">not</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> know.</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">Current conversation:</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">{history}</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">Human: {</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">input</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">}</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#D19A66;">AI</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>可以看到在这个会话模板里，{history}这一栏，就表示它会给到大模型历史的会话信息，{input}则用于存储这一次回话的用户输入，除此之外，我们还注意到，这个绘画模板试图告诉大模型&quot;If the AI does not know the answer to a question, it truthfully says it does not know.&quot;，如果他不知道，就回答不知道，请不要胡乱回答，以此来减少幻觉。关于具体的ConversationChain如何跟Memory组件来结合使用，在下面我们详细介绍记忆组件的时候看具体的例子</p><h2 id="_4-memory组件" tabindex="-1"><a class="header-anchor" href="#_4-memory组件"><span>4. Memory组件</span></a></h2><p>LangChain提供了多种Memory组件与ConversationChain结合使用过来完成会话记忆功能，这里主要介绍以下几种比较常见的Memory类型的使用方法</p><h3 id="_4-1-conversationbuffermemory" tabindex="-1"><a class="header-anchor" href="#_4-1-conversationbuffermemory"><span>4.1 ConversationBufferMemory</span></a></h3><p>ConversationBufferMemory ，即缓冲记忆组件。是 LangChain 中最简单直接的对话Memory组件。正如上面ConversationChain链内置的会话模板所描述的，Human和 AI 之间过去对话会以原始形式原封不动的传递给 <code>{history}</code> 参数，也就是它会把整个对话上下文作为提示<code>（prompt）</code>的一部分传给大模型。</p><p>首先创建一个ConversationChain实例，里面的Memory组件选择ConversationBufferMemory</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> langchain.chains.conversation.memory </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> ConversationBufferMemory</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">conversation_buf </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> ConversationChain</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    llm</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">llm,</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    memory</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">ConversationBufferMemory</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">()</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>紧接着输入第一轮会话</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">conversation_buf</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;早上好 AI!&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>程序输出</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">{</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&#39;input&#39;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&#39;早上好 AI!&#39;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &#39;history&#39;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&#39;&#39;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &#39;response&#39;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&#39;早上好！😊 今天有什么可以帮你的吗？无论是闲聊、解答问题，还是需要一些生活建议，我都很乐意帮忙～ 比如，你想聊聊天气、新闻，或者需要学习/工作上的小技巧？&#39;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">}</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><figure><img src="/assets/image-1-B6bmIt7M.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>可以看到，第一个会话中，history为空，这是因为在第一个会话之前，我们没有历史会话记录了，我们继续对话，输入只有当LLM考虑对话历史时才能回答的提示。还添加了一个 <code>count_tokens</code> 函数，以便我们可以看到每个会话交互使用了多少个token</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">count_tokens</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    conversation_buf, </span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">    &quot;在这里我感兴趣的是探索将大型语言模型与外部知识集成的可能性&quot;</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>程序输出</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">Spent a total of </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">749</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> tokens</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&#39;太棒了！将大型语言模型（LLM）与外部知识集成是一个非常有前景的方向，尤其是在增强模型的实时性、专业性和可解释性方面。以下是几种常见的集成方式及相关细节，供你探索：</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">---</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">### 1. **检索增强生成（RAG, Retrieval-Augmented Generation）**</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">   - **原理**：通过外部检索系统（如向量数据库）实时获取相关文档/数据，将其作为上下文输入模型，生成更准确的回答。</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">   - **优势**：  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">     - 突破模型训练数据的时效限制（例如回答最新新闻或领域动态）。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">     - 减少幻觉（Hallucination），答案可追溯来源。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">   - **工具示例**：  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">     - 向量数据库：Pinecone、Milvus、FAISS。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">     - 检索框架：LangChain的`RetrievalQA`、LlamaIndex。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">---</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">### 2. **API/工具调用（Function Calling）**</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">   - **原理**：模型通过API调用外部工具（如计算器、数据库、搜索引擎），动态获取信息。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">   - **应用场景**：  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">     - 实时数据：天气、股票、航班查询（如调用Wolfram Alpha或OpenWeatherMap）。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">     - 专业计算：数学求解、代码执行。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">   - **示例**：  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">     - OpenAI的`function calling`功能。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">     - Hugging Face的`Transformers Agents`。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">---</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">### 3. **微调（Fine-tuning）+ 领域知识注入**</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">   - **方法**：  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">     - 在特定领域数据（如医学、法律文本）上微调模型。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">     - 结合Adapter或LoRA技术，低成本适配新知识。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">   - **挑战**：需平衡通用能力与领域特异性，避免灾难性遗忘。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">---</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">### 4. **多模态扩展**</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">   - **集成非文本数据**：如图像、音频、传感器数据，通过多模态模型（如GPT-4V、Flamingo）处理复杂任务。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">   - **案例**：  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">     - 医疗诊断中结合影像与文本报告。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">     - 机器人通过语言模型解析环境传感器输入。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">---</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">### 5. **知识图谱（KG）结合**</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">   - **结构化知识**：将知识图谱作为外部存储，模型通过推理路径生成逻辑更严密的回答。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">   - **工具**：Neo4j、Amazon Neptune，配合图查询语言（如Cypher）。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">---</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">### 当前挑战</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">   - **延迟**：实时检索可能影响响应速度。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">   - **知识冲突**：如何协调模型内部知识与外部信源。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">   - **评估**：需设计新指标衡量“知识新鲜度”和集成效果。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">如果需要深入某个方向（比如代码示例或论文推荐），可以告诉我！你目前有具体的应用场景或技术栈吗？ 😊&#39;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><figure><img src="/assets/image-2-lDoh5kz5.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>接着输入会话内容</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">count_tokens</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    conversation_buf,</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">    &quot;我只是想分析不同的可能性。你能想到哪些？&quot;</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>输出</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">Spent a total of </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">1603</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> tokens</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&#39;当然可以！将大型语言模型（LLM）与外部知识集成的方式多种多样，每种方法都有其独特的适用场景和优缺点。以下是一个更全面的可能性分析框架，涵盖技术、应用和前沿探索方向：</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">---</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">### **一、按集成技术分类**</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">1. **动态检索型**  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">   - **RAG（检索增强生成）**：实时从数据库/网络检索信息，适合时效性强的场景（如客服、新闻摘要）。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">   - **搜索引擎联动**：直接调用Google/Bing API（如Perplexity AI的做法）。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">2. **静态知识注入型**  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">   - **微调（Fine-tuning）**：注入领域知识，但需定期重新训练。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">   - **知识蒸馏**：从专家模型或知识库中提取关键信息压缩到LLM中。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">3. **工具调用型**  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">   - **API/函数调用**：模型触发外部工具（如计算、数据库查询）。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">   - **插件系统**：如ChatGPT插件，连接Wolfram、Zapier等。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">4. **混合架构型**  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">   - **LLM + 知识图谱**：用图谱处理结构化关系（如药品相互作用查询）。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">   - **LLM + 数据库**：直接生成SQL查询或与NoDB交互。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">---</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">### **二、按知识类型分类**</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">1. **结构化知识**  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">   - 数据库（SQL/NoSQL）、知识图谱、Excel表格。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">   - *用例*：金融报表分析、供应链优化。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">2. **非结构化知识**  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">   - 文档（PDF/网页）、论文、会议记录。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">   - *用例*：法律合同审查、学术研究辅助。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">3. **多模态知识**  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">   - 图像、视频、音频（如CLIP或Whisper提取的嵌入）。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">   - *用例*：医疗影像描述、视频内容摘要。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">4. **实时数据流**  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">   - 传感器数据、社交媒体动态、IoT设备输入。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">   - *用例*：工厂设备故障诊断、舆情监控。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">---</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">### **三、按应用场景分类**</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">1. **垂直领域专业化**  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">   - 医疗（UpToDate临床指南集成）、法律（判例库查询）。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">   - *挑战*：领域术语准确性和合规性。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">2. **通用知识增强**  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">   - 结合维基百科、ArXiv等开放知识源，提升常识推理。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">3. **个性化交互**  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">   - 集成用户个人数据（邮件、日历）实现定制化回答（需隐私保护）。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">4. **创造性协作**  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">   - 链接设计素材库（如Canva）、音乐样本库辅助创作。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">---</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">### **四、前沿探索方向**</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">1. **主动学习型集成**  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">   - 模型自主判断何时需要检索外部知识（如Meta的“Toolformer”）。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">2. **自更新知识库**  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">   - 自动爬取/验证新知识并更新本地存储（如AutoGPT）。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">3. **联邦学习+知识集成**  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">   - 跨多个私有知识源协作，无需集中存储数据。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">4. **因果推理增强**  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">   - 结合因果模型（如DoWhy）提升逻辑一致性。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">5. **具身智能（Embodied AI）**  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">   - 机器人通过LLM理解环境知识（如PaLM-E）。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">---</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">### **五、潜在风险与权衡**</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">- **准确性 vs. 延迟**：检索需要时间，可能影响用户体验。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">- **可控性 vs. 灵活性**：严格的知识约束可能限制创造性。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">- **隐私 vs. 个性化**：医疗等敏感领域需数据隔离机制。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">如果需要，我可以针对某一类可能性展开具体案例或技术实现路径（比如“如何用LlamaIndex实现RAG”或“知识图谱的图神经网络结合方案”）——你对哪个维度最感兴趣？ 😊&#39;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><figure><img src="/assets/image-3-Bdt9hmyt.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>再接着问下一个问题</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">count_tokens</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    conversation_buf, </span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">    &quot;可以使用哪些类型的数据源来为模型提供上下文&quot;</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>输出</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">Spent a total of </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">2727</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> tokens</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&#39;为大型语言模型（LLM）提供上下文的数据源类型非常丰富，可以根据结构化程度、领域特异性、实时性等维度进行分类。以下是详细的分类及典型示例：</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">---</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">### **1. 结构化数据源**</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">**特点**：格式规范、易于机器解析，适合精准查询和逻辑推理。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">- **关系型数据库**：  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  - MySQL、PostgreSQL中的表格数据（如用户信息、交易记录）。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  - *用例*：生成SQL查询报告、电商订单分析。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">- **NoSQL数据库**：  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  - MongoDB（JSON文档）、Cassandra（宽列存储）。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  - *用例*：社交媒体的用户行为日志分析。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">- **知识图谱**：  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  - Neo4j、Amazon Neptune中的实体关系网络（如药品-副作用关联）。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  - *用例*：医疗诊断推理、企业知识管理。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">- **电子表格/CSV**：  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  - Excel、Google Sheets中的统计数据。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  - *用例*：财务预测、学术数据可视化。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">---</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">### **2. 非结构化数据源**</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">**特点**：自由文本或混合格式，需预处理（如嵌入、分块）才能利用。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">- **文档与书籍**：  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  - PDF、Word、Markdown文件（如产品手册、研究论文）。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  - *用例*：合同条款提取、自动摘要生成。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">- **网页内容**：  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  - 静态网页（Wikipedia）、动态页面（新闻网站）。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  - *工具*：Scrapy、BeautifulSoup爬虫 + Readability提取正文。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">- **学术资源**：  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  - ArXiv论文、PubMed医学摘要、专利数据库（如USPTO）。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  - *用例*：文献综述辅助、技术趋势分析。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">- **用户生成内容**：  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  - 论坛帖子（Reddit）、评论、社交媒体（Twitter）。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  - *挑战*：需过滤噪声和偏见。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">---</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">### **3. 实时/动态数据源**</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">**特点**：高频更新，需API或流式处理集成。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">- **搜索引擎**：  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  - Google/Bing的定制搜索API（时效性答案）。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">- **金融与市场数据**：  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  - 股票（Yahoo Finance）、加密货币（CoinGecko API）。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">- **天气与地理**：  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  - OpenWeatherMap、NASA Earthdata。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">- **物联网（IoT）流**：  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  - 传感器数据（温度、湿度）、工业设备日志。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">---</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">### **4. 多模态数据源**</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">**特点**：超越文本，需多模态模型处理。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">- **图像与视频**：  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  - 医学影像（DICOM）、监控视频帧 + 描述文本。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  - *工具*：CLIP、BLIP-2生成文本描述供LLM理解。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">- **音频与语音**：  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  - 会议录音（转文字后输入）、播客内容。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  - *工具*：Whisper ASR + LLM摘要。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">- **3D/传感器数据**：  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  - LiDAR点云、机器人环境地图（如ROS话题数据）。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">---</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">### **5. 私有/专有数据源**</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">**特点**：需权限管理，常需本地化部署。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">- **企业内网资源**：  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  - Confluence文档、Jira工单、Slack历史消息（需合规访问）。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">- **个人数据**：  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  - 电子邮件、日历事件（如Google Workspace集成）。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">- **行业数据库**：  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  - 医疗EMR系统（Epic）、法律Westlaw判例库。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">---</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">### **6. 合成/增强数据源**</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">**特点**：人工生成或增强的数据，用于特定场景。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">- **数据增强工具**：  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  - GPT生成模拟对话、Diffusion模型生成虚拟场景描述。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">- **仿真环境**：  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  - Unity/MuJoCo输出的虚拟机器人训练日志。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">---</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">### **选择数据源的关键考量**  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">1. **质量**：准确性、覆盖度、偏见（如社交媒体数据可能不中立）。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">2. **更新频率**：新闻需分钟级更新，百科全书可年更。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">3. **访问成本**：API调用费用（如Bloomberg Terminal） vs. 开源数据。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">4. **法律与伦理**：GDPR合规性、版权问题（如Scraping争议）。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">---</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">### **技术集成示例**  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">- **RAG流程**：  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  1. 用LlamaIndex将PDF分块并向量化。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  2. 存入Pinecone，查询时检索最相似的3个段落作为上下文。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">- **API调用**：  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  ```python  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  # 伪代码：天气API + LLM生成建议  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  weather = get_weather_api(location=&quot;北京&quot;)  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  prompt = f&quot;北京今天天气是{weather}，推荐穿什么衣服？&quot;  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  llm_response = generate(prompt)  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  ```  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">如果需要针对某一类数据源（如“如何高效解析PDF表格”或“实时流数据处理架构”）深入探讨，可以进一步展开！ 😊&#39;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><figure><img src="/assets/image-4-BddN9gFB.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>最后问一下我最初的目的是什么，看看大模型能否记得</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">count_tokens</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    conversation_buf, </span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">    &quot;我的目标是什么来着？&quot;</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>结果如下：</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">Spent a total of </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">3077</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> tokens</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&#39;根据我们之前的对话，你最初的目标是 **“探索将大型语言模型与外部知识集成的可能性”**，并进一步分析了不同类型的数据源如何为模型提供上下文。不过，如果你现在感觉方向有些模糊，我们可以重新梳理或调整目标。以下是可能的进展路径：</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">---</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">### **1. 回顾初始目标**</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">- **核心问题**：如何让LLM结合外部知识以增强能力？  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">- **已覆盖内容**：  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  - 集成方法（RAG、API调用、知识图谱等）。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  - 数据源类型（结构化、非结构化、实时等）。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">---</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">### **2. 可能的下一步方向**  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">根据你的兴趣，可以选择：  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">- **技术实现**：  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  - 动手实验某个方法（如用LangChain搭建RAG系统）。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  - 比较不同工具（LlamaIndex vs. Haystack）。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">- **场景深化**：  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  - 聚焦某一领域（如医疗、金融）的集成案例。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  - 研究如何解决特定问题（如减少幻觉、处理知识冲突）。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">- **理论探索**：  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  - 知识集成对模型评估指标的影响（如新鲜度、可解释性）。  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">---</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">### **3. 需要我帮你明确什么？**  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">- 是否想转向具体应用？  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">- 需要更基础的原理解释？  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">- 或者只是想自由发散讨论？  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">请随时告诉我你的想法，我们可以随时调整方向！ 😊  </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">（如果连初始目标也想调整，比如转向“如何评估集成效果”或“隐私保护方案”，也完全没问题～）&#39;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><figure><img src="/assets/image-5-5mFuWSXX.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>LLM 可以清楚地记住对话的历史，很正确的回答出我的初衷。接着看看 ConversationBufferMemory 是如何存储这个会话历史的</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(conversation_buf.memory.buffer)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>结果如下图所示</p><figure><img src="/assets/image-6-BVAX4G1z.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>可以看到，缓冲区直接保存了聊天历史中的每次交互。这种做法有几个优缺点</p><table><thead><tr><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td>存储所有会话，提供给大模型的信息很全面</td><td>更多token消耗，意味着响应时间变慢和成本增加</td></tr><tr><td>存储所有会话，简单直观</td><td>长对话无法记住，因为我们达到了LLM的token限制 text-davinci-003 和 gpt-3.5-turbo 的文本限制为 4096 个令牌）</td></tr></tbody></table><p>所以，整体而言，onversationBufferMemory 是一个很好的Memory入门选项，但受限于每次交互的存储</p><h3 id="_4-2-conversationbufferwindowmemory" tabindex="-1"><a class="header-anchor" href="#_4-2-conversationbufferwindowmemory"><span>4.2 ConversationBufferWindowMemory</span></a></h3><p>ConversationBufferWindowMemory，即缓冲窗口记忆组件， 是一种“滑动窗口式”的对话记忆方式。它的核心思想是：只保留最近的几轮人类和 AI 之间的对话记录，而不是把全部历史都存下来。相比之前的 ConversationBufferMemory，它多了一个参数参数k，意思是只记住最近 <code>k</code> 轮对话，然后“忘记”之前的互动。具体示例如下：</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> langchain.chains.conversation.memory </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> ConversationBufferWindowMemory</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">conversation_bufw </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> ConversationChain</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">        llm</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">llm,</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">        memory</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">ConversationBufferWindowMemory</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">k</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">1</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>在这个例子中，我们设置了 <code>k=1</code> ，这意味着窗口只会记住Human和 AI 之间的最新一次交互。而之前的聊天会话都将被抛弃。接下来还是进行跟刚刚onversationBufferMemory一样的几轮对话，看一下最终的效果</p><figure><img src="/assets/image-7-COlQ95aX.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>连续进行4轮对话之后，可以发现，当我再次问我的初衷，即我第一个问题的时候，大模型会把我上一个问题作为第一个问题给我回复。这是因为我们设置的ConversationBufferWindowMemory的参数k为1，所以大模型只会记住我们上一轮对话，如果想让大模型记住的内容更多一些，我们可以适当的调大这个参数k。</p><p>我们还可以这样看模型的有效“记忆”</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">bufw_history </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> conversation_bufw.memory.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">load_memory_variables</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    inputs</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">[]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)[</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&#39;history&#39;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(bufw_history)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>输出</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">Human: 我的目标是什么来着？</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#D19A66;">AI</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: 看起来你可能在对话中暂时忘记了最初的目标，这很正常！不过根据我们之前的讨论，你最初的问题是询问</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">**</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">“可以使用哪些类型的数据源来为模型提供上下文”</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">**</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">，而我已经详细列出了结构化</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">非结构化数据、公共</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">私有数据源、领域分类等技术细节。  </span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">如果这是你原本的目标，是否需要：  </span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">1</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">. </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">**</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">更聚焦的指导</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">**</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">？例如具体到某个行业（医疗</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">金融）的数据源选择？  </span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">2</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">. </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">**</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">技术实现建议</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">**</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">？比如如何将某种数据源（如数据库或API）接入LLM？  </span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">3</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">. </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">**</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">调整目标方向</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">**</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">？比如从数据源转向模型训练或应用场景？  </span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">或者，你现在的目标是否已经发生了变化？可以随时告诉我，我会根据你的新方向重新调整回答！ 🌟  </span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">（小提示：如果你完全忘记了上下文，可以尝试回顾聊天记录或描述当前困惑，我会帮你梳理</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">~</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">）</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>所以，如果我们只需要记忆最近的会话内容，ConversationBufferWindowMemory是一个很好的选择</p><h3 id="_4-3-conversationsummarymemory" tabindex="-1"><a class="header-anchor" href="#_4-3-conversationsummarymemory"><span>4.3 ConversationSummaryMemory</span></a></h3><p>ConversationSummaryMemory，即摘要记忆组件。为了避免过度使用token，我们除了可以使用ConversationBufferWindowMemory外，还可以使用 ConversationSummaryMemory 。使用ConversationBufferWindowMemory有一个缺陷，那就是只能记住最近几次会话，这对于需要长会话记忆的场景就不是用了。而ConversationSummaryMemory却可以弥补这个缺陷，ConversationSummaryMemory正如其名所示，这种形式的Memory在回答新问题的时候，对之前的问题进行了总结性的重述。ConversationSummaryMemory（对话总结记忆）的思路就是将对话历史进行汇总，然后再传递给 {history} 参数。具体示例如下：</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> langchain.chains.conversation.memory </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> ConversationSummaryMemory</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">conversation_sum </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> ConversationChain</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    llm</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">llm,</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    memory</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">ConversationSummaryMemory</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">llm</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">llm)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这里我们注意到，在使用 ConversationSummaryMemory 时，我们需要向对象传递一个LLM实例参数，因为历史会话的汇总功能由另一个 LLM 驱动的，这意味着对话的汇总实际上是由 AI 自己进行的。</p><p>我们可以通过以下方法看一下它在生成对话内容总结时所用的提示词模板</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(conversation_sum.memory.prompt.template)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>程序输出</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#D19A66;">EXAMPLE</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">Current summary:</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">The human asks what the </span><span style="--shiki-light:#383A42;--shiki-dark:#D19A66;">AI</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> thinks of artificial intelligence. The </span><span style="--shiki-light:#383A42;--shiki-dark:#D19A66;">AI</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> thinks artificial intelligence </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">is</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> a force </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">for</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> good.</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">New lines of conversation:</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">Human: Why do you think artificial intelligence </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">is</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> a force </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">for</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> good</span><span style="--shiki-light:white;--shiki-dark:#FFFFFF;">?</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#D19A66;">AI</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: Because artificial intelligence will </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">help</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> humans reach their full potential.</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">New summary:</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">The human asks what the </span><span style="--shiki-light:#383A42;--shiki-dark:#D19A66;">AI</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> thinks of artificial intelligence. The </span><span style="--shiki-light:#383A42;--shiki-dark:#D19A66;">AI</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> thinks artificial intelligence </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">is</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> a force </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">for</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> good because it will </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">help</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> humans reach their full potential.</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#D19A66;">END</span><span style="--shiki-light:#383A42;--shiki-dark:#D19A66;"> OF</span><span style="--shiki-light:#383A42;--shiki-dark:#D19A66;"> EXAMPLE</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">Current summary:</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">{summary}</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">New lines of conversation:</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">{new_lines}</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">New summary:</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这个模板的大致意思是：</p><blockquote><p>请对提供的对话内容进行逐步总结，在已有摘要的基础上添加新的总结内容</p></blockquote><p>其中 <code>{summary}</code> 是之前的摘要，<code>{new_lines}</code> 是新增的对话，模型根据这两部分内容生成 <code>NEW_SUMMARY</code>。</p><p>然后，我们用ConversationSummaryMemory再进行一次前几轮的对话，看一下最终的效果</p><figure><img src="/assets/image-8-z-yPPPVX.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>可以看到，用这种Memory组件，大模型能够“记住”我们的原始目标，给出我们想要的答复，我们可以再看一下会话总结的具体内容</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(conversation_sum.memory.buffer)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>输出如下图所示：</p><figure><img src="/assets/image-9-DxDNU-4O.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>使用这种方法，在回答完最后一个问题是，最终我们在消耗了1953个token，而使用ConversationBufferMemory最终我们消耗了3077个token，可见token数量确实减少了。那是否用ConversationSummaryMemory就一定比ConversationBufferMemory好呢？</p><p>答案是：<strong>不一定，</strong>对于较长的历史胡会话，使用会话总结可能会有效减少token，而对于本身会话历史就很少，再用ConversationSummaryMemory可能反而会增加token的消耗。</p><p>下图是一个token消耗随会话次数变化的关联曲线图，反映了ConversationBufferMemory与ConversationSummaryMemory的token计数（y 轴）随交互次数（x 轴）增加的变化趋势</p><figure><img src="/assets/image-10-CworfVcL.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>假设我们有一个很长的会话，ConversationSummaryMemory最初使用的标记数量要多得多。然而，随着对话的进行，ConversationSummaryMemory消耗的token增长速度逐渐减慢。相比之下，ConversationBufferMemory继续与聊天中的token数量线性增长。</p><p><strong>ConversationSummaryMemory 的优势在于，它通过对对话内容进行总结摘要，有效降低了 Token 的使用量，从而能保留更多轮的历史记录，非常适合处理多轮、长时间的对话。同时，它的使用方式也相对直观。</strong></p><p>然而，它也存在一些局限。对于较短的对话，摘要机制反而可能带来额外的 Token 消耗，不划算。此外，它的记忆效果高度依赖于用于摘要的 LLM 的质量和表现，而摘要本身也需要消耗额外的 Token，这在一定程度上增加了成本。</p><p>虽然 ConversationSummaryMemory 能通过压缩对话历史来优化 Token 管理，适用于长对话场景，但它并不能完全突破大模型上下文长度的限制——随着对话的不断推进，最终还是可能触及模型的最大上下文边界。</p><h3 id="_4-4-conversationsummarybuffermemory" tabindex="-1"><a class="header-anchor" href="#_4-4-conversationsummarybuffermemory"><span>4.4 ConversationSummaryBufferMemory</span></a></h3><p>ConversationSummaryBufferMemory ，即摘要缓存记忆组件，其实是 ConversationSummaryMemory 和ConversationBufferWindowMemory 的混合，它旨在在对话中总结早期的互动，同时尽量保留最近互动中的原始内容。它有一个<code>max_token_limit</code>参数，当最新的对话文字长度在 <code>max_token_limit</code> 以内的时候，LangChain 会记忆原始对话内容；当对话文字超出了这个参数的长度，那么模型就会把所有超过预设长度的内容进行总结，以节省 Token 数量</p><p>由于国内模型在langchain的集成中对token计数功能封装的不是很好，比如langchain_deepseek就不支持，所以这里为了方便，就直接选用OpenApi模型来测试</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> langchain.memory </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> ConversationSummaryBufferMemory</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> langchain.chains </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> ConversationChain</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> langchain_community.chat_models </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> ChatOpenAI</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> dotenv </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> load_dotenv</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> os</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 加载环境变量</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">load_dotenv</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">()</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 获取 API 密钥</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">openai_api_key </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> os.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">getenv</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;OPENAI_API_KEY&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 创建 LLM 实例</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">llm </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> ChatOpenAI</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    model</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;gpt-3.5-turbo&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    temperature</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">0</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    openai_api_key</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">openai_api_key</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 创建对话记忆</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">memory </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> ConversationSummaryBufferMemory</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    llm</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">llm,</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    max_token_limit</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">300</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 创建对话链</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">conversation_sum_buf </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> ConversationChain</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    llm</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">llm,</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    memory</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">memory,</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>紧接着还是进行上面例子中的几轮对话</p><figure><img src="/assets/image-11-CAQwVKgf.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>可以看到，大模型还是给了我们正确的回复。我们在打印一下它存储的历史会话记录，看看它是如何存储的</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(conversation_sum_buf.memory.buffer)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>输出</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">System: The human greets the </span><span style="--shiki-light:#383A42;--shiki-dark:#D19A66;">AI</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;"> in</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> Chinese </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">and</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> discusses integrating large language models </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">with</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> external knowledge. The </span><span style="--shiki-light:#383A42;--shiki-dark:#D19A66;">AI</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> suggests using knowledge graphs, multimodal learning, transfer learning, </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">and</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> reinforcement learning to enhance the model</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&#39;s knowledge base. The human asks about using different types of data sources to provide context, and the AI recommends using text, image, video, audio, and structured data sources. By combining these diverse data sources, the model can better understand the world and make accurate predictions.</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">Human: 我的目标是什么来着？</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#D19A66;">AI</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: 您的目标是根据您的输入和需求来获取有关大型语言模型和外部知识集成的信息。您希望了解如何利用知识图谱、多模态学习、迁移学习和强化学习来增强模型的知识库。您还询问了关于使用不同类型的数据源来提供上下文的问题，我建议使用文本、图像、视频、音频和结构化数据源。通过结合这些多样化的数据源，模型可以更好地理解世界并做出准确的预测。您的目标是获取关于这些主题的详细信息吗？</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>可以看到它完整的只保存了我们最后一次会话记录，这是因为我们把<code>max_token_limit</code>设置的非常小，只设置为300，但是它为什么还能知道我们最初的第一次会话内容，准确回答呢？因为这些信息被ConversationSummaryBufferMemory中的“摘要”组件捕获，尽管它被“缓冲窗口”组件遗漏了</p><p>虽然在使用上，ConversationSummaryBufferMemory需要调整要总结的内容和缓冲窗口内要保留的内容，但 ConversationSummaryBufferMemory 却给了我们很多灵活性，并且是我们迄今为止唯一一种可以记住遥远交互并以原始形式存储最近交互的内存类型</p><figure><img src="/assets/image-12-BSQPcVAz.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>上图是随着会话次数的增加，各种记忆组件消耗tokens的数量关系曲线，可以看到尽管ConversationSummaryBufferMemory既包括历史会话的摘要，有包括最近的会话原始记录，蛋它的 token 计数增加与其他方法相比还是很有竞争力的</p><h3 id="_4-5-其他会话记忆组件" tabindex="-1"><a class="header-anchor" href="#_4-5-其他会话记忆组件"><span>4.5 其他会话记忆组件</span></a></h3><p>除了上述提到的 Memory 方式之外，LangChain 的官方网站<a href="https://api.python.langchain.com/en/latest/langchain/memory.html%E4%B8%AD%E8%BF%98%E6%8F%90%E4%BE%9B%E4%BA%86%E4%B8%80%E4%BA%9B%E5%85%B6%E4%BB%96%E7%9A%84%E8%AE%B0%E5%BF%86%E6%96%B9%E5%BC%8F%EF%BC%8C%E6%AF%94%E5%A6%82ConversationKnowledgeGraphMemory" target="_blank" rel="noopener noreferrer">https://api.python.langchain.com/en/latest/langchain/memory.html中还提供了一些其他的记忆方式，比如ConversationKnowledgeGraphMemory</a> 和 ConversationEntityMemory，这里就不再做过多赘述，感兴趣的可以去官网详细看看的具它们体用法。</p><h2 id="_5-小结" tabindex="-1"><a class="header-anchor" href="#_5-小结"><span>5. 小结</span></a></h2><p>LangChain 框架中的 Memory 组件，它是构建具有上下文感知能力的应用（如聊天机器人）的关键。由于大型语言模型（LLM）本身是无状态的，Memory 组件通过存储和管理会话历史，从而让大模型具备了“记忆力”。并且LangChain提供了多种Memory，具体选择哪种 Memory 组件，取决于具体的应用场景，需要综合考虑对话长度、上下文保留的详细程度、Token 成本以及响应时间等因素。</p><p>总之，LangChain 的 Memory 组件为开发者提供了强大的工具集，可以根据不同场景灵活选择和配置，从而构建出更加智能、自然和拥有持续记忆能力的语言模型应用</p></div><!----><footer class="vp-page-meta"><div class="vp-meta-item edit-link"><a class="auto-link external-link vp-meta-label" href="https://github.com/yourusername/xiucai-stack/edit/main/src/AI进阶之路/大模型应用开发/LangChain从入门到精通/Memery.md" aria-label="在 GitHub 上编辑此页" rel="noopener noreferrer" target="_blank"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="edit icon" name="edit"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->在 GitHub 上编辑此页<!----></a></div><div class="vp-meta-item git-info"><div class="update-time"><span class="vp-meta-label">上次编辑于: </span><span class="vp-meta-info" data-allow-mismatch="text">2025/6/26 23:42:17</span></div><div class="contributors"><span class="vp-meta-label">贡献者: </span><!--[--><!--[--><span class="vp-meta-info" title="email: 380059082@qq.com">gupeng</span><!--]--><!--]--></div></div></footer><nav class="vp-page-nav"><a class="route-link auto-link prev" href="/AI%E8%BF%9B%E9%98%B6%E4%B9%8B%E8%B7%AF/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/LangChain%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/%E9%93%BE.html" aria-label="链"><div class="hint"><span class="arrow start"></span>上一页</div><div class="link"><!---->链</div></a><a class="route-link auto-link next" href="/AI%E8%BF%9B%E9%98%B6%E4%B9%8B%E8%B7%AF/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/LangChain%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/RAG(%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA).html" aria-label="RAG(检索增强)"><div class="hint">下一页<span class="arrow end"></span></div><div class="link">RAG(检索增强)<!----></div></a></nav><div id="comment" class="giscus-wrapper input-top vp-comment" vp-comment style="display:block;"><div class="loading-icon-wrapper" style="display:flex;align-items:center;justify-content:center;height:96px"><svg xmlns="http://www.w3.org/2000/svg" width="48" height="48" preserveAspectRatio="xMidYMid" viewBox="25 25 50 50"><animateTransform attributeName="transform" type="rotate" dur="2s" keyTimes="0;1" repeatCount="indefinite" values="0;360"></animateTransform><circle cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="4" stroke-linecap="round"><animate attributeName="stroke-dasharray" dur="1.5s" keyTimes="0;0.5;1" repeatCount="indefinite" values="1,200;90,200;1,200"></animate><animate attributeName="stroke-dashoffset" dur="1.5s" keyTimes="0;0.5;1" repeatCount="indefinite" values="0;-35px;-125px"></animate></circle></svg></div></div><!----><!--]--></main><!--]--><footer class="vp-footer-wrapper" vp-footer><div class="vp-footer">秀才的进阶之路｜专注于技术分享与知识传播｜<a href="https://beian.miit.gov.cn/" target="_blank" style="color: #3b82f6; text-decoration: none;">粤ICP备2024352247号</a></div><div class="vp-copyright">Copyright © 2025 秀才 </div></footer></div><!--]--><!--[--><!----><!--[--><!--]--><!--]--><!--]--></div>
    <script type="module" src="/assets/app-DJgFtDFQ.js" defer></script>
  </body>
</html>
